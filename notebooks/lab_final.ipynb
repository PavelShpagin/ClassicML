{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# China Real Estate Demand Prediction - Lab Report\n",
        "\n",
        "## Competition Overview\n",
        "This notebook presents a comprehensive solution to the Kaggle competition \"China Real Estate Demand Prediction\". The goal is to predict monthly new house transaction amounts across 96 sectors in China.\n",
        "\n",
        "**Final Leaderboard Score: 0.56248** (Geometric Mean + Seasonality)\n",
        "\n",
        "### Evaluation Metric\n",
        "The competition uses a custom two-stage MAPE-based metric:\n",
        "1. Calculate MAPE for each prediction\n",
        "2. Compute `good_rate` = % of predictions with APE ≤ 100%\n",
        "3. If `good_rate` < 0.3: score = 0\n",
        "4. Otherwise: score = `good_rate` × mean(1/(1 + MAPE) for good predictions)\n",
        "\n",
        "This metric heavily penalizes predictions when true values are zero or near-zero.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Add parent directory to path for imports\n",
        "import sys\n",
        "ROOT = Path.cwd().parent\n",
        "sys.path.append(str(ROOT))\n",
        "\n",
        "from src.data import DatasetPaths, load_all_training_tables, load_test, prepare_train_target, explode_test_id\n",
        "from src.features import build_time_lagged_features\n",
        "from src.models import competition_score\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all training data\n",
        "paths = DatasetPaths(root_dir=str(ROOT))\n",
        "train_data = load_all_training_tables(paths)\n",
        "\n",
        "print(\"Available training tables:\")\n",
        "for name, df in train_data.items():\n",
        "    print(f\"  {name}: {df.shape}\")\n",
        "\n",
        "# Load test data\n",
        "test_df = load_test(paths)\n",
        "print(f\"\\nTest data shape: {test_df.shape}\")\n",
        "\n",
        "# Prepare target variable\n",
        "target_wide, target_long = prepare_train_target(train_data['new_house_transactions'])\n",
        "print(f\"\\nTarget matrix shape: {target_wide.shape} (months × sectors)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Target Variable Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze target distribution\n",
        "all_values = target_wide.values.flatten()\n",
        "all_values = all_values[~np.isnan(all_values)]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Distribution of all values\n",
        "axes[0].hist(all_values, bins=50, edgecolor='black', alpha=0.7)\n",
        "axes[0].set_title('Distribution of Transaction Amounts')\n",
        "axes[0].set_xlabel('Amount')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_yscale('log')\n",
        "\n",
        "# Distribution of non-zero values\n",
        "non_zero = all_values[all_values > 0]\n",
        "axes[1].hist(np.log1p(non_zero), bins=50, edgecolor='black', alpha=0.7, color='green')\n",
        "axes[1].set_title('Log Distribution of Non-Zero Values')\n",
        "axes[1].set_xlabel('log(Amount + 1)')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "\n",
        "# Zero vs Non-zero\n",
        "zero_counts = [(all_values == 0).sum(), (all_values > 0).sum()]\n",
        "axes[2].pie(zero_counts, labels=['Zero', 'Non-Zero'], autopct='%1.1f%%', colors=['red', 'green'])\n",
        "axes[2].set_title('Zero vs Non-Zero Values')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Statistics:\")\n",
        "print(f\"  Total values: {len(all_values)}\")\n",
        "print(f\"  Zero values: {zero_counts[0]} ({100*zero_counts[0]/len(all_values):.1f}%)\")\n",
        "print(f\"  Mean (non-zero): {non_zero.mean():.2f}\")\n",
        "print(f\"  Median (non-zero): {np.median(non_zero):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Benchmarking Results\n",
        "\n",
        "### 2.1 Baseline Models (Ridge Regression)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load benchmark results\n",
        "ridge_results = pd.read_csv(ROOT / 'reports' / 'cv_ridge_results.csv')\n",
        "advanced_results = pd.read_csv(ROOT / 'reports' / 'cv_advanced_models.csv')\n",
        "\n",
        "# Display Ridge results\n",
        "print(\"Ridge Regression Results (Time Series CV):\")\n",
        "print(ridge_results.to_string(index=False))\n",
        "\n",
        "# Plot Ridge hyperparameter curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].plot(ridge_results['alpha'], ridge_results['score_mean'], 'o-', label='Score')\n",
        "axes[0].fill_between(ridge_results['alpha'], \n",
        "                     ridge_results['score_mean'] - ridge_results['score_std'],\n",
        "                     ridge_results['score_mean'] + ridge_results['score_std'],\n",
        "                     alpha=0.3)\n",
        "axes[0].set_xlabel('Alpha (Regularization)')\n",
        "axes[0].set_ylabel('Competition Score')\n",
        "axes[0].set_xscale('log')\n",
        "axes[0].set_title('Ridge: Score vs Alpha')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(ridge_results['alpha'], ridge_results['mape_mean'], 'o-', color='red', label='MAPE')\n",
        "axes[1].set_xlabel('Alpha (Regularization)')\n",
        "axes[1].set_ylabel('MAPE')\n",
        "axes[1].set_xscale('log')\n",
        "axes[1].set_title('Ridge: MAPE vs Alpha')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Advanced Models (Gradient Boosting)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display advanced model results\n",
        "print(\"Advanced Models Results (Time Series CV):\")\n",
        "print(advanced_results.to_string(index=False))\n",
        "\n",
        "# Compare models\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Score comparison\n",
        "models = advanced_results['model'].values\n",
        "scores = advanced_results['score_mean'].values\n",
        "axes[0].bar(models, scores, color=['blue', 'green', 'orange'])\n",
        "axes[0].set_ylabel('Competition Score')\n",
        "axes[0].set_title('Model Comparison: Competition Score')\n",
        "axes[0].set_ylim(0, max(scores) * 1.1)\n",
        "for i, v in enumerate(scores):\n",
        "    axes[0].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
        "\n",
        "# RMSE comparison\n",
        "rmse = advanced_results['rmse_mean'].values\n",
        "axes[1].bar(models, rmse, color=['blue', 'green', 'orange'])\n",
        "axes[1].set_ylabel('RMSE')\n",
        "axes[1].set_title('Model Comparison: RMSE')\n",
        "for i, v in enumerate(rmse):\n",
        "    axes[1].text(i, v + 500, f'{v:.0f}', ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Best model\n",
        "best_idx = advanced_results['score_mean'].idxmax()\n",
        "print(f\"\\nBest Model: {advanced_results.loc[best_idx, 'model']}\")\n",
        "print(f\"  Score: {advanced_results.loc[best_idx, 'score_mean']:.4f} ± {advanced_results.loc[best_idx, 'score_std']:.4f}\")\n",
        "print(f\"  RMSE: {advanced_results.loc[best_idx, 'rmse_mean']:.0f}\")\n",
        "print(f\"  MAPE: {advanced_results.loc[best_idx, 'mape_mean']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Bayesian Optimization with Optuna\n",
        "\n",
        "We performed Bayesian hyperparameter optimization using Optuna for XGBoost:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Optuna results\n",
        "import json\n",
        "with open(ROOT / 'reports' / 'xgb_optuna_best.json', 'r') as f:\n",
        "    optuna_results = json.load(f)\n",
        "\n",
        "print(\"Optuna Bayesian Optimization Results:\")\n",
        "print(f\"Best Score: {optuna_results['best_score']:.4f}\")\n",
        "print(\"\\nOptimized Hyperparameters:\")\n",
        "for param, value in optuna_results['best_params'].items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {param}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"  {param}: {value}\")\n",
        "\n",
        "# Compare with baseline\n",
        "baseline_xgb_score = advanced_results[advanced_results['model'] == 'XGBoost']['score_mean'].values[0]\n",
        "improvement = (optuna_results['best_score'] - baseline_xgb_score) / baseline_xgb_score * 100\n",
        "\n",
        "print(f\"\\nImprovement over baseline XGBoost: {improvement:.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Key Insights and Challenges\n",
        "\n",
        "### 4.1 Data Challenges\n",
        "- **15.5% zero values** in the target variable\n",
        "- **Missing sector 95** in training data but present in test data\n",
        "- **High variance** in transaction amounts (50 to 606,407)\n",
        "\n",
        "### 4.2 Metric Challenges\n",
        "The competition metric severely penalizes predictions when true values are zero:\n",
        "- If APE > 100% for more than 30% of predictions → Score = 0\n",
        "- Predicting any positive value when true is 0 → Infinite APE\n",
        "\n",
        "### 4.3 Solutions Implemented\n",
        "1. **Time-lagged features**: lag_1, lag_2, lag_3, lag_6, lag_12\n",
        "2. **Rolling statistics**: 3, 6, 12-month rolling means and geometric means\n",
        "3. **Zero detection**: Random Forest classifier to identify zero transactions\n",
        "4. **Ensemble methods**: XGBoost, LightGBM, CatBoost\n",
        "5. **Bayesian optimization**: Optuna for hyperparameter tuning\n",
        "6. **Conservative predictions**: Heavy dampening to avoid large errors\n",
        "\n",
        "## 5. Conclusion\n",
        "\n",
        "This competition demonstrates the challenges of real-world time series prediction with:\n",
        "- Sparse data with many zeros\n",
        "- Strict evaluation metrics\n",
        "- Missing sectors in training data\n",
        "\n",
        "Our best local CV score was ~0.50 using XGBoost with Optuna tuning. The main difficulty lies in handling zero values correctly - the metric's design makes it extremely challenging to achieve high scores when the data contains many zero transactions.\n",
        "\n",
        "### Future Improvements\n",
        "1. More sophisticated zero detection models\n",
        "2. Sector-specific models for high-variance sectors\n",
        "3. External data incorporation (economic indicators, seasonality)\n",
        "4. Advanced time series models (LSTM, Prophet)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Final Results - Kaggle Leaderboard\n",
        "\n",
        "After extensive experimentation with various approaches, we achieved our best results using proven baseline methods from the competition examples:\n",
        "\n",
        "### Submission Results\n",
        "\n",
        "| Method | Public Score | Description |\n",
        "|--------|-------------|-------------|\n",
        "| **Geometric Mean + Seasonality** | **0.56248** | Best score - Geometric mean with December boost |\n",
        "| Geometric Mean Baseline | 0.55528 | 6-month geometric mean with zero guard |\n",
        "| Simple Median | 0.21591 | Conservative median-based approach |\n",
        "| XGBoost (Optuna-tuned) | 0.00000 | Complex model failed due to zero predictions |\n",
        "| XGBoost (Basic) | 0.00000 | Failed due to metric sensitivity |\n",
        "| Ridge Regression | 0.00000 | Linear model couldn't handle zeros |\n",
        "\n",
        "### Key Success Factors\n",
        "\n",
        "1. **Geometric Mean**: More robust than arithmetic mean for skewed distributions\n",
        "2. **Zero Guard**: Critical - if any of last 6 months was zero, predict zero\n",
        "3. **Seasonality**: December typically shows 30% higher transactions\n",
        "4. **Simplicity**: Complex models failed due to metric's extreme sensitivity\n",
        "\n",
        "### Why Complex Models Failed\n",
        "\n",
        "The competition's two-stage metric severely penalizes any prediction with APE > 100%, especially when true values are zero. Our XGBoost and Ridge models, despite good local CV scores (~0.35-0.50), scored 0.0 on the leaderboard because:\n",
        "- They occasionally predicted non-zero for zero sectors\n",
        "- The penalty for these errors overwhelmed any gains from accurate predictions\n",
        "- The metric requires extremely conservative predictions\n",
        "\n",
        "### Final Approach Details\n",
        "\n",
        "```python\n",
        "# Winning approach: Geometric Mean + Seasonality\n",
        "1. Calculate 6-month geometric mean for each sector\n",
        "2. Apply zero guard (if any recent month = 0, predict 0)  \n",
        "3. Boost December predictions by 1.3x based on historical patterns\n",
        "4. Result: 0.56248 public score\n",
        "```\n",
        "\n",
        "This competition demonstrates that understanding the evaluation metric is crucial - sometimes simpler, more conservative approaches outperform complex machine learning models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final implementation - Geometric Mean with Seasonality\n",
        "# This approach achieved 0.56248 on the public leaderboard\n",
        "\n",
        "def geometric_mean_with_seasonality(train_data, test_data):\n",
        "    \"\"\"\n",
        "    Implements the winning approach: geometric mean with December seasonality boost.\n",
        "    \"\"\"\n",
        "    # Parameters (tuned based on competition examples)\n",
        "    t1 = 6  # months for geometric mean\n",
        "    t2 = 6  # months to check for zeros\n",
        "    december_boost = 1.3  # boost factor for December\n",
        "    \n",
        "    # Calculate base predictions\n",
        "    last_months = train_data.tail(t1)\n",
        "    geo_mean = np.exp(np.log(last_months.replace(0, np.nan)).mean(axis=0, skipna=True))\n",
        "    geo_mean = geo_mean.fillna(0)\n",
        "    \n",
        "    # Apply zero guard\n",
        "    zero_mask = train_data.tail(t2).min(axis=0) == 0\n",
        "    geo_mean[zero_mask] = 0\n",
        "    \n",
        "    # Apply December boost for December predictions\n",
        "    predictions = []\n",
        "    for month, sector in test_data:\n",
        "        pred = geo_mean[sector]\n",
        "        if month == 'December':\n",
        "            pred *= december_boost\n",
        "        predictions.append(pred)\n",
        "    \n",
        "    return predictions\n",
        "\n",
        "print(\"Winning approach implemented successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
